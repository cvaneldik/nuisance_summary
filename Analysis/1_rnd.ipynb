{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0630400a-d653-4a99-81e6-b3daa41cbb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset:\n",
      "/home/wecapstor1/caph/mppi045h/nuisance_summary/PKS_flare/HESS_public/dataset-simulated-2.154434690031884-hr.fits.gz\n",
      "\n",
      "========================================================================================================================\n",
      "0\n",
      "========================================================================================================================\n",
      "nn 43\n",
      "shift [0.], tilt [0.],  bias [0.], res [0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model names must be unique",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:155\u001b[0m\n",
      "File \u001b[0;32m/home/wecapstor1/caph/mppi045h/nuisance_summary/Analysis/../Dataset_Setup.py:135\u001b[0m, in \u001b[0;36mSetup.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# adding systematics if set before and setting irf/piecewise model to the dataset_N\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_irf_sys:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_irf_systematic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtilt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_irf_model(dataset_N)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bkg_sys:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# sets the counts\u001b[39;00m\n",
      "File \u001b[0;32m/home/wecapstor1/caph/mppi045h/nuisance_summary/Analysis/../Dataset_Setup.py:255\u001b[0m, in \u001b[0;36mSetup.add_irf_systematic\u001b[0;34m(self, bias, resolution, norm, tilt)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_irf_systematic\u001b[39m(\u001b[38;5;28mself\u001b[39m, bias, resolution, norm, tilt):\n\u001b[1;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    sets IRF model , sets the model parameters as the input, sets the exposure and the edisp according to input\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    removes the IRF model again\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_irf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_helper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_helper\u001b[38;5;241m.\u001b[39mirf_model\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m bias\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_helper\u001b[38;5;241m.\u001b[39mirf_model\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m resolution\n",
      "File \u001b[0;32m/home/wecapstor1/caph/mppi045h/nuisance_summary/Analysis/../Dataset_Setup.py:234\u001b[0m, in \u001b[0;36mSetup.set_irf_model\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    232\u001b[0m IRFmodels\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    233\u001b[0m models \u001b[38;5;241m=\u001b[39m Models(dataset\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m--> 234\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIRFmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m dataset\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m models\n",
      "File \u001b[0;32m/home/wecapstor1/caph/mppi045h/anaconda3/envs/gammapy-dev/lib/python3.8/_collections_abc.py:962\u001b[0m, in \u001b[0;36mMutableSequence.append\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS.append(value) -- append value to the end of the sequence\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wecapstor1/caph/mppi045h/gammapy/gammapy/modeling/models/core.py:1142\u001b[0m, in \u001b[0;36mModels.insert\u001b[0;34m(self, idx, model)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, model):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames:\n\u001b[0;32m-> 1142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel names must be unique\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models\u001b[38;5;241m.\u001b[39minsert(idx, model)\n",
      "\u001b[0;31mValueError\u001b[0m: Model names must be unique"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def save():\n",
    "    with open(f\"{path}/data/1_P_draw_info.txt\", \"a\") as myfile:\n",
    "        info = str(float(shift_rnd[0])) + '    '+ str(float(tilt_rnd[0])) + '    '\n",
    "        info += str(float(bias_rnd[0])) + '    '+ str(float(res_rnd[0])) + '    '\n",
    "        info +=  str(float(dataset.stat_sum())) + '\\n'\n",
    "        myfile.write(info)\n",
    "    with open(f\"{path}/data/1_P_draw_par.txt\", \"a\") as myfile:\n",
    "        myfile.write(stri + '\\n')\n",
    "    with open(f\"{path}/data/1_P_draw_flux.txt\", \"a\") as myfile:\n",
    "        myfile.write( ff + '\\n')\n",
    "    with open(f\"{path}/data/1_P_draw_flux2e.txt\", \"a\") as myfile:\n",
    "        myfile.write( ff2 + '\\n')\n",
    "\n",
    "    with open(f\"{path}/data/1_N_P_draw_par.txt\", \"a\") as myfile:\n",
    "        myfile.write(stri_N + '\\n')\n",
    "    with open(f\"{path}/data/1_N_P_draw_flux.txt\", \"a\") as myfile:\n",
    "        myfile.write( ffN + '\\n')\n",
    "    with open(f\"{path}/data/1_N_P_draw_flux2e.txt\", \"a\") as myfile:\n",
    "        myfile.write( ffN2 + '\\n')\n",
    "        \n",
    "def computing_contour(dataset, note):\n",
    "        \n",
    "    results = []\n",
    "    for parname1, parname2 in parameter_names :\n",
    "        numpoints = 5\n",
    "        print( parname1, parname2, numpoints)\n",
    "        result = fit_cor.stat_contour(dataset,\n",
    "                             dataset.models.parameters[parname1],\n",
    "                             dataset.models.parameters[parname2],\n",
    "                              numpoints=numpoints \n",
    "                                      \n",
    "                            )\n",
    "\n",
    "        contour_write = dict()\n",
    "        for k in result.keys():\n",
    "            print(k)\n",
    "            if k != \"success\":\n",
    "                contour_write[k] = result[k].tolist()\n",
    "        import yaml\n",
    "        with open(f\"{path}/data/contours/{note}_{parname1}_{parname2}.yml\", \"w\") as outfile:\n",
    "            yaml.dump(contour_write, outfile, default_flow_style=False)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "import gammapy \n",
    "import pyximport\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "import sys\n",
    "from gammapy.modeling import Fit, Parameter, Parameters\n",
    "from gammapy.modeling.models import Models\n",
    "from gammapy.maps import MapAxis\n",
    "from gammapy.modeling.models.spectral import scale_plot_flux\n",
    "from gammapy.estimators import  FluxPointsEstimator\n",
    "from gammapy.modeling.models import IRFModels, EffAreaIRFModel, ERecoIRFModel\n",
    "    \n",
    "sys.path.append('/home/katrin/Documents/nuisance_summary/')\n",
    "sys.path.append('../')\n",
    "import Dataset_load \n",
    "from  Dataset_Setup import Setup, GaussianCovariance_matrix\n",
    "\n",
    "\n",
    "c = Dataset_load.load_config()\n",
    "awo, aw, ewo, ew = c['_colors']  \n",
    "\n",
    "livetimes = c['livetimes']\n",
    "livetime = c['livetime']\n",
    "sys = c['sys']\n",
    "norm = c['norm'] \n",
    "tilt = c['tilt'] \n",
    "bias =  c['bias'] \n",
    "resolution = c['resolution'] \n",
    "path = f\"../{c['folder']}\"\n",
    "parameter_names = c['parameter_names']        \n",
    "\n",
    "#for live in livetimes[8:]:\n",
    "for live in [livetime]:\n",
    "\n",
    "    dataset_asimov = Dataset_load.create_asimov(\n",
    "        model=c['model'], source=c['source'], parameters=None,\n",
    "        livetime = f\"{live}-hr\"\n",
    "    )\n",
    "\n",
    "    mask = dataset_asimov.mask.data.sum(axis=2).sum(axis=1)>0\n",
    "    ebins = dataset_asimov.counts.geom.axes[0].center[mask]\n",
    "\n",
    "\n",
    "    N = 1\n",
    "    save_flux = True\n",
    "    save_fluxpoints = 1\n",
    "    save_fluxpoints_N = 1\n",
    "    dataset_N = True\n",
    "    contour = 0\n",
    "    zero_sys = 0\n",
    "\n",
    "\n",
    "    for n in range(N):\n",
    "        print()\n",
    "        print('====' * 30)\n",
    "        print(n)\n",
    "        print('====' * 30)\n",
    "        res_rnd = np.random.normal(0, resolution, 1)\n",
    "        bias_rnd =  np.random.normal(0, bias, 1)\n",
    "        shift_rnd = np.random.normal(0, norm, 1)\n",
    "        tilt_rnd = np.random.normal(0, tilt, 1)\n",
    "        nn = np.random.randint(0,100)\n",
    "        print(\"nn\", nn)\n",
    "        \n",
    "        if zero_sys:\n",
    "            shift_rnd, tilt_rnd = np.array([0.]), np.array([0.])\n",
    "            bias_rnd, res_rnd = np.array([0.]), np.array([0.])\n",
    "        \n",
    "        print(f\"shift {shift_rnd}, tilt {tilt_rnd},  bias {bias_rnd}, res {res_rnd}\")\n",
    "        setup = Setup(dataset_input=dataset_asimov, rnd = True)\n",
    "        setup.set_up_irf_sys(bias_rnd, res_rnd, shift_rnd, tilt_rnd)\n",
    "\n",
    "        dataset, dataset_N = setup.run()\n",
    "        # irf model\n",
    "        # happens in set_up_irf_sys\n",
    "        # setup.set_irf_model(dataset_N)\n",
    "        if sys == \"Eff_area\":\n",
    "            dataset_N.models.parameters['resolution'].frozen = True\n",
    "            dataset_N.models.parameters['bias'].frozen = True\n",
    "            dataset_N.irf_model.parameters['tilt'].frozen = False\n",
    "            dataset_N.irf_model.parameters['norm'].frozen = False\n",
    "            dataset_N.e_reco_n = 10\n",
    "            setup.set_irf_prior(dataset_N, bias, resolution, norm, tilt)\n",
    "            \n",
    "            \n",
    "        if sys == \"E_reco\":\n",
    "            dataset_N.models.parameters['resolution'].frozen = True\n",
    "            dataset_N.models.parameters['bias'].frozen = False\n",
    "            dataset_N.irf_model.parameters['tilt'].frozen = True\n",
    "            dataset_N.irf_model.parameters['norm'].frozen = True\n",
    "            setup.set_irf_prior(dataset_N, bias, resolution, norm, tilt)\n",
    "        \n",
    "        if sys == \"Combined\":\n",
    "            dataset_N.models.parameters['resolution'].frozen = True\n",
    "            dataset_N.models.parameters['bias'].frozen = False\n",
    "            dataset_N.irf_model.parameters['tilt'].frozen = False\n",
    "            dataset_N.irf_model.parameters['norm'].frozen = False\n",
    "            setup.set_irf_prior(dataset_N, bias, resolution, norm, tilt)\n",
    "            \n",
    "        if sys == \"BKG\":\n",
    "            magnitude = c['magnitude']\n",
    "            corrlength = c['corrlength']\n",
    "            # piece wise model\n",
    "            # remove old bkg model\n",
    "            setup.set_up_bkg_sys_V( breake = 10,\n",
    "                                index1 = 2,\n",
    "                                index2 = 1.5, \n",
    "                                magnitude = magnitude )\n",
    "\n",
    "            dataset_asimov, dataset_asimov_N = setup.run()\n",
    "\n",
    "            setup.unset_model(dataset_asimov_N, FoVBackgroundModel)\n",
    "            setup.set_piecewise_bkg_model(dataset_asimov_N)\n",
    "            # energy of the following parameters smaller than ethrshold\n",
    "            dataset_asimov_N.background_model.parameters['norm0'].frozen = True\n",
    "            dataset_asimov_N.background_model.parameters['norm1'].frozen = True\n",
    "            dataset_asimov_N.background_model.parameters['norm2'].frozen = True\n",
    "            dataset_asimov_N.background_model.parameters['norm3'].frozen = True\n",
    "            setup.set_bkg_prior(dataset_asimov_N, magnitude, corrlength)\n",
    "            frozen_pos = 1\n",
    "            if frozen_pos:\n",
    "                dataset_asimov.models.parameters['lon_0'].frozen = True\n",
    "                dataset_asimov.models.parameters['lat_0'].frozen = True\n",
    "                dataset_asimov_N.models.parameters['lon_0'].frozen = True\n",
    "                dataset_asimov_N.models.parameters['lat_0'].frozen = True\n",
    "        \n",
    "        fit_cor = Fit(store_trace=False)\n",
    "        dataset.plot_residuals()\n",
    "        result_cor = fit_cor.run([dataset])\n",
    "        print(\"fit w/o nui ended:\")\n",
    "        print(result_cor)\n",
    "        print(dataset.models)\n",
    "\n",
    "        stri = \"\"\n",
    "        parameters =  ['amplitude', 'index', 'lambda_', 'norm', 'tilt']\n",
    "        if \"crab_break\" in c['model']:\n",
    "            parameters =  ['amplitude', 'index1', 'index2', 'ebreak', 'beta', 'norm', 'tilt']\n",
    "        if \"crab_log\" in c['model']:\n",
    "            parameters =  ['amplitude', 'alpha', 'beta', 'norm', 'tilt']\n",
    "            \n",
    "        for p in parameters:\n",
    "            stri += str(dataset.models.parameters[p].value)  + '   ' +  str(dataset.models.parameters[p].error)  + '   '\n",
    "        stri += str(live) + \"  \"\n",
    "        print(stri)\n",
    "\n",
    "\n",
    "        fluxes = []\n",
    "        for e in ebins:\n",
    "            flux =  dataset.models[0].spectral_model(e)\n",
    "            fluxes.append(flux.value)\n",
    "\n",
    "        ff = str()\n",
    "        for f in fluxes:\n",
    "            ff += str(f) + \"  \"\n",
    "        #print(ff)\n",
    "\n",
    "        energy_bounds = (ebins[0], ebins[-1] ) * u.TeV\n",
    "\n",
    "        energy_min, energy_max = energy_bounds\n",
    "        energy = MapAxis.from_energy_bounds(\n",
    "            energy_min,\n",
    "            energy_max,\n",
    "            len(ebins),\n",
    "        )\n",
    "        fluxe2, _ = dataset.models[0].spectral_model._get_plot_flux(sed_type='dnde', energy=energy)\n",
    "        fluxe2 = scale_plot_flux(fluxe2, energy_power=2)\n",
    "        fluxe2 = fluxe2.quantity[:, 0, 0]\n",
    "        fluxe2 = np.array(fluxe2)   \n",
    "        ff2 = str()\n",
    "        for f in fluxe2:\n",
    "            ff2 += str(f) + \"  \"\n",
    "\n",
    "        energy_edges = dataset.geoms['geom'].axes[0].edges[::2]\n",
    "\n",
    "        fit_cor = Fit(store_trace=False)\n",
    "        result_cor = fit_cor.run([dataset_N])\n",
    "        print()\n",
    "        print(\"fit with nui ended:\")\n",
    "        print(result_cor)\n",
    "        print(dataset_N.models)\n",
    "\n",
    "\n",
    "        stri_N = \"\"\n",
    "        [parameters.append(p) for p in ['norm', 'tilt', 'bias', 'resolution']]\n",
    "        for p in parameters:\n",
    "            stri_N += str(dataset_N.models.parameters[p].value)  + '   ' +  str(dataset_N.models.parameters[p].error)  + '   '\n",
    "        stri_N += str(live) + \"  \"\n",
    "        print(stri_N)\n",
    "\n",
    "        fluxes = []\n",
    "        for e in ebins:\n",
    "            flux =  dataset_N.models[0].spectral_model(e)\n",
    "            fluxes.append(flux.value)\n",
    "\n",
    "        ffN = str()\n",
    "        for f in fluxes:\n",
    "            ffN += str(f) + \"  \"\n",
    "\n",
    "        energy_bounds = (ebins[0], ebins[-1] ) * u.TeV\n",
    "\n",
    "        energy_min, energy_max = energy_bounds\n",
    "        energy = MapAxis.from_energy_bounds(\n",
    "            energy_min,\n",
    "            energy_max,\n",
    "            len(ebins),\n",
    "        )\n",
    "\n",
    "        fluxe2, _ = dataset_N.models[0].spectral_model._get_plot_flux(sed_type='dnde', energy=energy)\n",
    "        fluxe2 = scale_plot_flux(fluxe2, energy_power=2)\n",
    "        fluxe2 = fluxe2.quantity[:, 0, 0]\n",
    "        fluxe2 = np.array(fluxe2)   \n",
    "        ffN2 = str()\n",
    "        for f in fluxe2:\n",
    "            ffN2 += str(f) + \"  \"\n",
    "\n",
    "        rnds = f\"{shift_rnd[0]:.6}_{tilt_rnd[0]:.6}_{bias_rnd[0]:.6}_{res_rnd[0]:.6}\"\n",
    "        if save_fluxpoints:\n",
    "            print(\"computing Fluxpoints\")\n",
    "            dataset.models.parameters['amplitude'].scan_n_sigma  = 5\n",
    "            dataset_N.models.parameters['amplitude'].scan_n_sigma  = 5\n",
    "\n",
    "            esti  = FluxPointsEstimator(energy_edges= energy_edges, \n",
    "                                        selection_optional =  \"all\"\n",
    "                                       )\n",
    "            fluxpoints = esti.run([dataset])\n",
    "            # freeze all but IRF for fp and reopt = True\n",
    "            dataset_N.models[0].parameters.freeze_all()\n",
    "            dataset_N.models[0].parameters['amplitude'].frozen = False\n",
    "            dataset_N.background_model.parameters.freeze_all()\n",
    "            esti  = FluxPointsEstimator(energy_edges= energy_edges, selection_optional =[ \"ul\"],# \"errn-errp\", \"all\",\n",
    "                                       reoptimize=True)\n",
    "            fluxpoints_N = esti.run([dataset_N])\n",
    "            fluxpoints_N.write(f'{path}/data/fluxpoints/1P_fluxpoints_N_{live}_{rnds}_{nn}.fits',\n",
    "                              overwrite = True)\n",
    "            dataset_N.models.write(f'{path}/data/fluxpoints/1P_model_N_{live}_{rnds}_{nn}.yaml',\n",
    "                                  overwrite = True)\n",
    "            fluxpoints.write(f'{path}/data/fluxpoints/1P_fluxpoints_{live}_{rnds}_{nn}.fits',\n",
    "                            overwrite = True)\n",
    "            dataset.models.write(f'{path}/data/fluxpoints/1P_model_{live}_{rnds}_{nn}.yaml',\n",
    "                                overwrite = True)\n",
    "            with open(f\"{path}/data/fluxpoints/1P_draw_fluxpoints.txt\", \"a\") as myfile:\n",
    "                myfile.write(str(nn) + '\\n')\n",
    "        if contour:\n",
    "            computing_contour(dataset, rnds)\n",
    "            print(\"N\")\n",
    "            computing_contour(dataset_N, \"N\"+rnds)\n",
    "            with open(f\"{path}/data/contours/1_P_draw_info.txt\", \"a\") as myfile:\n",
    "                info = str(float(shift_rnd[0])) + '    '+ str(float(tilt_rnd[0])) + '    '\n",
    "                info += str(float(bias_rnd[0])) + '    '+ str(float(res_rnd[0])) + '    '\n",
    "                info +=  str(float(dataset.stat_sum())) + '\\n'\n",
    "                myfile.write(info)\n",
    "            with open(f\"{path}/data/contours/1_P_draw_par.txt\", \"a\") as myfile:\n",
    "                myfile.write(stri + '\\n')\n",
    "\n",
    "            with open(f\"{path}/data/contours/1_N_P_draw_par.txt\", \"a\") as myfile:\n",
    "                myfile.write(stri_N + '\\n')\n",
    "\n",
    "        if zero_sys == False and contour ==False: # else only the fluxpoints and models are saved but not the info\n",
    "            save()\n",
    "        plotting = 0\n",
    "        if plotting:\n",
    "            print(\"in Plotting\")\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            ep = 2\n",
    "            ax = dataset_asimov.models[0].spectral_model.plot((0.1,100)*u.TeV, color = 'tab:blue',\n",
    "                                                      label = \"without nui\",\n",
    "                                                      energy_power = ep)\n",
    "\n",
    "\n",
    "            dataset.models[0].spectral_model.plot((0.1,100)*u.TeV,ax = ax, color = 'black',\n",
    "                                                        energy_power = ep)\n",
    "\n",
    "            dataset_N.models[0].spectral_model.plot((0.1,100)*u.TeV,ax = ax, color = 'tab:orange',\n",
    "                                                   label = \"with nui\",\n",
    "                                                   energy_power = ep)\n",
    "            dataset_N.models[0].spectral_model.plot_error((0.1,100)*u.TeV,ax = ax, facecolor = 'tab:orange',\n",
    "                                                         energy_power = ep)\n",
    "            #dataset.models[0].spectral_model.plot_error((0.1,100)*u.TeV,ax = ax, facecolor = 'tab:blue',\n",
    "            #                                           energy_power = ep)\n",
    "            try:\n",
    "                fluxpoints.plot(ax =ax, energy_power = ep)\n",
    "                fluxpoints_N.plot(ax =ax,energy_power = ep )\n",
    "            except:\n",
    "                kk = 0\n",
    "            ax.legend(title = f\"live: {live:.3} hr\\n norm:{shift_rnd[0]:.3}\\n tilt:{tilt_rnd[0]:.3}\\n bias:{bias_rnd[0]:.3}\")\n",
    "            fig = plt.gcf()\n",
    "            fig.savefig(f\"{path}/data/fluxpoints/plots/{live}_{rnds}_{nn}.png\")\n",
    "            #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b00e0-2b04-4d35-ac6d-a4e342d0111d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
